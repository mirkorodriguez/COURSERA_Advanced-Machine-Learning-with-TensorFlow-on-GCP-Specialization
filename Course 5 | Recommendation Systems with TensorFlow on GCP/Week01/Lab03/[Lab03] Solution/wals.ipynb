{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering on Google Analytics data\n",
    "\n",
    "This notebook demonstrates how to implement a WALS matrix refactorization approach to do collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'qwiklabs-gcp-ef189eb62756ef5e' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'qwiklabs-gcp-ef189eb62756ef5e' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# do not change these\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create raw dataset\n",
    "<p>\n",
    "For collaborative filtering, we don't need to know anything about either the users or the content. Essentially, all we need to know is userId, itemId, and rating that the particular user gave the particular item.\n",
    "<p>\n",
    "In this case, we are working with newspaper articles. The company doesn't ask their users to rate the articles. However, we can use the time-spent on the page as a proxy for rating.\n",
    "<p>\n",
    "Normally, we would also add a time filter to this (\"latest 7 days\"), but our dataset is itself limited to a few days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitorId</th>\n",
       "      <th>contentId</th>\n",
       "      <th>session_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7337153711992174438</td>\n",
       "      <td>100074831</td>\n",
       "      <td>44652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5190801220865459604</td>\n",
       "      <td>100170790</td>\n",
       "      <td>1214205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5874973374932455844</td>\n",
       "      <td>100510126</td>\n",
       "      <td>32109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2293633612703952721</td>\n",
       "      <td>100510126</td>\n",
       "      <td>47744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1173698801255170595</td>\n",
       "      <td>100676857</td>\n",
       "      <td>10512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             visitorId  contentId  session_duration\n",
       "0  7337153711992174438  100074831             44652\n",
       "1  5190801220865459604  100170790           1214205\n",
       "2  5874973374932455844  100510126             32109\n",
       "3  2293633612703952721  100510126             47744\n",
       "4  1173698801255170595  100676857             10512"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "\n",
    "sql=\"\"\"\n",
    "#standardSQL\n",
    "WITH visitor_page_content AS (\n",
    "\n",
    "   SELECT  \n",
    "     fullVisitorID,\n",
    "     (SELECT MAX(IF(index=10, value, NULL)) FROM UNNEST(hits.customDimensions)) AS latestContentId,  \n",
    "     (LEAD(hits.time, 1) OVER (PARTITION BY fullVisitorId ORDER BY hits.time ASC) - hits.time) AS session_duration \n",
    "   FROM `cloud-training-demos.GA360_test.ga_sessions_sample`,   \n",
    "     UNNEST(hits) AS hits\n",
    "   WHERE \n",
    "     # only include hits on pages\n",
    "      hits.type = \"PAGE\"\n",
    "\n",
    "   GROUP BY   \n",
    "     fullVisitorId, latestContentId, hits.time\n",
    "     )\n",
    "\n",
    "# aggregate web stats\n",
    "SELECT   \n",
    "  fullVisitorID as visitorId,\n",
    "  latestContentId as contentId,\n",
    "  SUM(session_duration) AS session_duration \n",
    " \n",
    "FROM visitor_page_content\n",
    "  WHERE latestContentId IS NOT NULL \n",
    "  GROUP BY fullVisitorID, latestContentId\n",
    "  HAVING session_duration > 0\n",
    "  ORDER BY latestContentId \n",
    "\"\"\"\n",
    "\n",
    "df = bq.Query(sql).execute().result().to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.789140e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.272183e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.346436e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.709425e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.793750e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.293928e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.690598e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       session_duration\n",
       "count      2.789140e+05\n",
       "mean       1.272183e+05\n",
       "std        2.346436e+05\n",
       "min        1.000000e+00\n",
       "25%        1.709425e+04\n",
       "50%        5.793750e+04\n",
       "75%        1.293928e+05\n",
       "max        7.690598e+06"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = df.describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_duration</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.789140e+05</td>\n",
       "      <td>278914.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.272183e+05</td>\n",
       "      <td>0.402428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.346436e+05</td>\n",
       "      <td>0.349948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.709425e+04</td>\n",
       "      <td>0.088514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.793750e+04</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.293928e+05</td>\n",
       "      <td>0.669995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.690598e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       session_duration         rating\n",
       "count      2.789140e+05  278914.000000\n",
       "mean       1.272183e+05       0.402428\n",
       "std        2.346436e+05       0.349948\n",
       "min        1.000000e+00       0.000005\n",
       "25%        1.709425e+04       0.088514\n",
       "50%        5.793750e+04       0.300000\n",
       "75%        1.293928e+05       0.669995\n",
       "max        7.690598e+06       1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the rating is the session_duration scaled to be in the range 0-1.  This will help with training.\n",
    "df['rating'] = 0.3 * (1 + (df['session_duration'] - stats.loc['50%', 'session_duration'])/stats.loc['50%', 'session_duration'])\n",
    "df.loc[df['rating'] > 1, 'rating'] = 1\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['session_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf data\n",
    "mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/collab_raw.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7337153711992174438,100074831,0.23120776699029122\r\n",
      "5190801220865459604,100170790,1.0\r\n",
      "5874973374932455844,100510126,0.16626019417475726\r\n",
      "2293633612703952721,100510126,0.24721812297734627\r\n",
      "1173698801255170595,100676857,0.054431067961165036\r\n",
      "7262596434087807556,100735153,0.23134239482200647\r\n",
      "883397426232997550,10083328,0.9450355987055016\r\n",
      "1808867070685560283,100906145,1.0\r\n",
      "7615995624631762562,100906145,0.4823145631067961\r\n",
      "5519169380728479914,100915139,0.19948737864077667\r\n"
     ]
    }
   ],
   "source": [
    "!head data/collab_raw.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset for WALS\n",
    "<p>\n",
    "The raw dataset (above) won't work for WALS:\n",
    "<ol>\n",
    "<li> The userId and itemId have to be 0,1,2 ... so we need to create a mapping from visitorId (in the raw data) to userId and contentId (in the raw data) to itemId.\n",
    "<li> We will need to save the above mapping to a file because at prediction time, we'll need to know how to map the contentId in the table above to the itemId.\n",
    "<li> We'll need two files: a \"rows\" dataset where all the items for a particular user are listed; and a \"columns\" dataset where all the users for a particular item are listed.\n",
    "</ol>\n",
    "\n",
    "<p>\n",
    "\n",
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def create_mapping(values, filename):\n",
    "  with open(filename, 'w') as ofp:\n",
    "    value_to_id = {value:idx for idx, value in enumerate(values.unique())}\n",
    "    for value, idx in value_to_id.items():\n",
    "      ofp.write('{},{}\\n'.format(value, idx))\n",
    "  return value_to_id\n",
    "\n",
    "df = pd.read_csv('data/collab_raw.csv',\n",
    "                 header=None,\n",
    "                 names=['visitorId', 'contentId', 'rating'],\n",
    "                dtype={'visitorId': str, 'contentId': str, 'rating': np.float})\n",
    "df.to_csv('data/collab_raw.csv', index=False, header=False)\n",
    "user_mapping = create_mapping(df['visitorId'], 'data/users.csv')\n",
    "item_mapping = create_mapping(df['contentId'], 'data/items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> data/collab_raw.csv <==\r\n",
      "7337153711992174438,100074831,0.2312077669902912\r\n",
      "5190801220865459604,100170790,1.0\r\n",
      "5874973374932455844,100510126,0.16626019417475724\r\n",
      "\r\n",
      "==> data/items.csv <==\r\n",
      "236941062,1402\r\n",
      "299786600,4672\r\n",
      "243874813,1540\r\n",
      "\r\n",
      "==> data/users.csv <==\r\n",
      "1672553935406836733,2849\r\n",
      "3588200922529727879,41146\r\n",
      "7789950935531938708,60125\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['userId'] = df['visitorId'].map(user_mapping.get)\n",
    "df['itemId'] = df['contentId'].map(item_mapping.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>itemId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.231208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.166260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.247218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.054431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  itemId    rating\n",
       "0       0       0  0.231208\n",
       "1       1       1  1.000000\n",
       "2       2       2  0.166260\n",
       "3       3       2  0.247218\n",
       "4       4       3  0.054431"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_df = df[['userId', 'itemId', 'rating']]\n",
    "mapped_df.to_csv('data/collab_mapped.csv', index=False, header=False)\n",
    "mapped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating rows and columns datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>itemId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.231208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.166260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.247218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.054431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  itemId    rating\n",
       "0       0       0  0.231208\n",
       "1       1       1  1.000000\n",
       "2       2       2  0.166260\n",
       "3       3       2  0.247218\n",
       "4       4       3  0.054431"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "mapped_df = pd.read_csv('data/collab_mapped.csv', header=None, names=['userId', 'itemId', 'rating'])\n",
    "mapped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5668 items, 82802 users, 278914 interactions\n"
     ]
    }
   ],
   "source": [
    "NITEMS = np.max(mapped_df['itemId']) + 1\n",
    "NUSERS = np.max(mapped_df['userId']) + 1\n",
    "mapped_df['rating'] = np.round(mapped_df['rating'].values, 2)\n",
    "print('{} items, {} users, {} interactions'.format( NITEMS, NUSERS, len(mapped_df) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([0]), array([0.23]))\n",
      "(1, array([1]), array([1.]))\n",
      "(2, array([2, 3]), array([0.17, 0.25]))\n",
      "(3, array([4]), array([0.05]))\n",
      "(4, array([5]), array([0.23]))\n",
      "(5, array([6]), array([0.95]))\n"
     ]
    }
   ],
   "source": [
    "grouped_by_items = mapped_df.groupby('itemId')\n",
    "iter = 0\n",
    "for item, grouped in grouped_by_items:\n",
    "  print(item, grouped['userId'].values, grouped['rating'].values)\n",
    "  iter = iter + 1\n",
    "  if iter > 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "grouped_by_items = mapped_df.groupby('itemId')\n",
    "with tf.python_io.TFRecordWriter('data/users_for_item') as ofp:\n",
    "  for item, grouped in grouped_by_items:\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "          'key': tf.train.Feature(int64_list=tf.train.Int64List(value=[item])),\n",
    "          'indices': tf.train.Feature(int64_list=tf.train.Int64List(value=grouped['userId'].values)),\n",
    "          'values': tf.train.Feature(float_list=tf.train.FloatList(value=grouped['rating'].values))\n",
    "        }))\n",
    "    ofp.write(example.SerializeToString())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_users = mapped_df.groupby('userId')\n",
    "with tf.python_io.TFRecordWriter('data/items_for_user') as ofp:\n",
    "  for user, grouped in grouped_by_users:\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "          'key': tf.train.Feature(int64_list=tf.train.Int64List(value=[user])),\n",
    "          'indices': tf.train.Feature(int64_list=tf.train.Int64List(value=grouped['itemId'].values)),\n",
    "          'values': tf.train.Feature(float_list=tf.train.FloatList(value=grouped['rating'].values))\n",
    "        }))\n",
    "    ofp.write(example.SerializeToString())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_users = mapped_df.groupby('userId')\n",
    "N = 0\n",
    "with tf.python_io.TFRecordWriter('data/items_for_user_subset') as ofp:\n",
    "  for user, grouped in grouped_by_users:\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "          'key': tf.train.Feature(int64_list=tf.train.Int64List(value=[user])),\n",
    "          'indices': tf.train.Feature(int64_list=tf.train.Int64List(value=grouped['itemId'].values)),\n",
    "          'values': tf.train.Feature(float_list=tf.train.FloatList(value=grouped['rating'].values))\n",
    "        }))\n",
    "    ofp.write(example.SerializeToString())    \n",
    "    N = N + 1\n",
    "    if N > 20:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 31828\r\n",
      "-rw-r--r-- 1 root root 13116591 Apr 12 00:10 collab_raw.csv\r\n",
      "-rw-r--r-- 1 root root  2131923 Apr 12 00:10 users.csv\r\n",
      "-rw-r--r-- 1 root root    82193 Apr 12 00:10 items.csv\r\n",
      "-rw-r--r-- 1 root root  7789342 Apr 12 00:11 collab_mapped.csv\r\n",
      "-rw-r--r-- 1 root root  2244886 Apr 12 00:11 users_for_item\r\n",
      "-rw-r--r-- 1 root root  7207031 Apr 12 00:11 items_for_user\r\n",
      "-rw-r--r-- 1 root root     1747 Apr 12 00:11 items_for_user_subset\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lrt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we created the following data files from collab_raw.csv:\n",
    "<ol>\n",
    "<li> ```collab_mapped.csv``` is essentially the same data as in ```collab_raw.csv``` except that ```visitorId``` and ```contentId``` which are business-specific have been mapped to ```userId``` and ```itemId``` which are enumerated in 0,1,2,....  The mappings themselves are stored in ```items.csv``` and ```users.csv``` so that they can be used during inference.\n",
    "<li> ```users_for_item``` contains all the users/ratings for each item in TFExample format\n",
    "<li> ```items_for_user``` contains all the items/ratings for each user in TFExample format\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with WALS\n",
    "\n",
    "Once you have the dataset, do matrix factorization with WALS using the [WALSMatrixFactorization](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/factorization/WALSMatrixFactorization) in the contrib directory.\n",
    "This is an estimator model, so it should be relatively familiar.\n",
    "<p>\n",
    "As usual, we write an input_fn to provide the data to the model, and then create the Estimator to do train_and_evaluate.\n",
    "Because it is in contrib and hasn't moved over to tf.estimator yet, we use tf.contrib.learn.Experiment to handle the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.contrib.factorization import WALSMatrixFactorization\n",
    "  \n",
    "def read_dataset(mode, args):\n",
    "  def decode_example(protos, vocab_size):\n",
    "    features = {'key': tf.FixedLenFeature([1], tf.int64),\n",
    "                'indices': tf.VarLenFeature(dtype=tf.int64),\n",
    "                'values': tf.VarLenFeature(dtype=tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(protos, features)\n",
    "    values = tf.sparse_merge(parsed_features['indices'], parsed_features['values'], vocab_size=vocab_size)\n",
    "    # Save key to remap after batching\n",
    "    # This is a temporary workaround to assign correct row numbers in each batch.\n",
    "    # You can ignore details of this part and remap_keys().\n",
    "    key = parsed_features['key']\n",
    "    decoded_sparse_tensor = tf.SparseTensor(indices=tf.concat([values.indices, [key]], axis = 0), values = tf.concat([values.values, [0.0]], axis = 0), dense_shape = values.dense_shape)\n",
    "    return decoded_sparse_tensor\n",
    "  \n",
    "  \n",
    "  def remap_keys(sparse_tensor):\n",
    "    # Current indices of our SparseTensor that we need to fix\n",
    "    bad_indices = sparse_tensor.indices\n",
    "    # Current values of our SparseTensor that we need to fix\n",
    "    bad_values = sparse_tensor.values \n",
    "  \n",
    "    # Group by the batch_indices and get the count for each  \n",
    "    size = tf.segment_sum(data = tf.ones_like(bad_indices[:,0], dtype = tf.int64), segment_ids = bad_indices[:,0]) - 1\n",
    "    # The number of batch_indices (this should be batch_size unless it is a partially full batch)\n",
    "    length = tf.shape(size, out_type = tf.int64)[0]\n",
    "    # Finds the cumulative sum which we can use for indexing later\n",
    "    cum = tf.cumsum(size)\n",
    "    # The offsets between each example in the batch due to our concatentation of the keys in the decode_example method\n",
    "    length_range = tf.range(start = 0, limit = length, delta = 1, dtype = tf.int64)\n",
    "    # Indices of the SparseTensor's indices member of the rows we added by the concatentation of our keys in the decode_example method\n",
    "    cum_range = cum + length_range\n",
    "\n",
    "    # The keys that we have extracted back out of our concatentated SparseTensor\n",
    "    gathered_indices = tf.squeeze(tf.gather(bad_indices, cum_range)[:,1])\n",
    "\n",
    "    # The enumerated row indices of the SparseTensor's indices member\n",
    "    sparse_indices_range = tf.range(tf.shape(bad_indices, out_type = tf.int64)[0], dtype = tf.int64)\n",
    "\n",
    "    # We want to find here the row indices of the SparseTensor's indices member that are of our actual data and not the concatentated rows\n",
    "    # So we want to find the intersection of the two sets and then take the opposite of that\n",
    "    x = sparse_indices_range\n",
    "    s = cum_range\n",
    "\n",
    "    # Number of multiples we are going to tile x, which is our sparse_indices_range\n",
    "    tile_multiples = tf.concat([tf.ones(tf.shape(tf.shape(x)), dtype=tf.int64), tf.shape(s, out_type = tf.int64)], axis = 0)\n",
    "    # Expands x, our sparse_indices_range, into a rank 2 tensor and then multiplies the rows by 1 (no copying) and the columns by the number of examples in the batch\n",
    "    x_tile = tf.tile(tf.expand_dims(x, -1), tile_multiples)\n",
    "    # Essentially a vectorized logical or, that we then negate\n",
    "    x_not_in_s = ~tf.reduce_any(tf.equal(x_tile, s), -1)\n",
    "\n",
    "    # The SparseTensor's indices that are our actual data by using the boolean_mask we just made above applied to the entire indices member of our SparseTensor\n",
    "    selected_indices = tf.boolean_mask(tensor = bad_indices, mask = x_not_in_s, axis = 0)\n",
    "    # Apply the same boolean_mask to the entire values member of our SparseTensor to get the actual values data\n",
    "    selected_values = tf.boolean_mask(tensor = bad_values, mask = x_not_in_s, axis = 0)\n",
    "\n",
    "    # Need to replace the first column of our selected_indices with keys, so we first need to tile our gathered_indices\n",
    "    tiling = tf.tile(input = tf.expand_dims(gathered_indices[0], -1), multiples = tf.expand_dims(size[0] , -1))\n",
    "    \n",
    "    # We have to repeatedly apply the tiling to each example in the batch\n",
    "    # Since it is jagged we cannot use tf.map_fn due to the stacking of the TensorArray, so we have to create our own custom version\n",
    "    def loop_body(i, tensor_grow):\n",
    "      return i + 1, tf.concat(values = [tensor_grow, tf.tile(input = tf.expand_dims(gathered_indices[i], -1), multiples = tf.expand_dims(size[i] , -1))], axis = 0)\n",
    "\n",
    "    _, result = tf.while_loop(lambda i, tensor_grow: i < length, loop_body, [tf.constant(1, dtype = tf.int64), tiling])\n",
    "    \n",
    "    # Concatenate tiled keys with the 2nd column of selected_indices\n",
    "    selected_indices_fixed = tf.concat([tf.expand_dims(result, -1), tf.expand_dims(selected_indices[:, 1], -1)], axis = 1)\n",
    "    \n",
    "    # Combine everything together back into a SparseTensor\n",
    "    remapped_sparse_tensor = tf.SparseTensor(indices = selected_indices_fixed, values = selected_values, dense_shape = sparse_tensor.dense_shape)\n",
    "    return remapped_sparse_tensor\n",
    "\n",
    "    \n",
    "  def parse_tfrecords(filename, vocab_size):\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None # indefinitely\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    "    \n",
    "    files = tf.gfile.Glob(os.path.join(args['input_path'], filename))\n",
    "    \n",
    "    # Create dataset from file list\n",
    "    dataset = tf.data.TFRecordDataset(files)\n",
    "    dataset = dataset.map(lambda x: decode_example(x, vocab_size))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(args['batch_size'])\n",
    "    dataset = dataset.map(lambda x: remap_keys(x))\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "  \n",
    "  def _input_fn():\n",
    "    features = {\n",
    "      WALSMatrixFactorization.INPUT_ROWS: parse_tfrecords('items_for_user', args['nitems']),\n",
    "      WALSMatrixFactorization.INPUT_COLS: parse_tfrecords('users_for_item', args['nusers']),\n",
    "      WALSMatrixFactorization.PROJECT_ROW: tf.constant(True)\n",
    "    }\n",
    "    return features, None\n",
    "  \n",
    "  # just for developing line by line. You don't need this in production\n",
    "  def _input_fn_subset():\n",
    "    features = {\n",
    "      WALSMatrixFactorization.INPUT_ROWS: parse_tfrecords('items_for_user_subset', args['nitems']),\n",
    "      WALSMatrixFactorization.PROJECT_ROW: tf.constant(True)\n",
    "    }\n",
    "    return features, None\n",
    "  \n",
    "  def input_cols():\n",
    "    return parse_tfrecords('users_for_item', args['nusers'])\n",
    "  \n",
    "  return _input_fn#_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is helpful in developing the input function. You don't need it in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_out():\n",
    "  with tf.Session() as sess:\n",
    "    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n",
    "                    {'input_path': 'data', 'batch_size': 8, 'nitems': 5668, 'nusers': 82802})\n",
    "    feats, _ = fn()\n",
    "    print(feats['input_rows'].eval())\n",
    "\n",
    "try_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k(user, item_factors, k):\n",
    "  all_items = tf.matmul(tf.expand_dims(user, 0), tf.transpose(item_factors))\n",
    "  topk = tf.nn.top_k(all_items, k=k)\n",
    "  return tf.cast(topk.indices, dtype=tf.int64)\n",
    "    \n",
    "def batch_predict(args):\n",
    "  import numpy as np\n",
    "  with tf.Session() as sess:\n",
    "    estimator = tf.contrib.factorization.WALSMatrixFactorization(\n",
    "                                                                 num_rows=args['nusers'], num_cols=args['nitems'],\n",
    "                                                                 embedding_dimension=args['n_embeds'],\n",
    "                                                                 model_dir=args['output_dir'])\n",
    "    \n",
    "    # this is how you would get the row factors for out-of-vocab user data\n",
    "    #row_factors = list(estimator.get_projections(input_fn=read_dataset(tf.estimator.ModeKeys.EVAL, args)))\n",
    "    #user_factors = tf.convert_to_tensor(np.array(row_factors))\n",
    "    \n",
    "    # but for in-vocab data, the row factors are already in the checkpoint\n",
    "    user_factors = tf.convert_to_tensor(estimator.get_row_factors()[0]) # (nusers, nembeds)\n",
    "    # in either case, we have to assume catalog doesn't change, so col_factors are read in\n",
    "    item_factors = tf.convert_to_tensor(estimator.get_col_factors()[0])# (nitems, nembeds)\n",
    "    \n",
    "    # for each user, find the top K items\n",
    "    topk = tf.squeeze(tf.map_fn(lambda user: find_top_k(user, item_factors, args['topk']), user_factors, dtype=tf.int64))\n",
    "    with file_io.FileIO(os.path.join(args['output_dir'], 'batch_pred.txt'), mode='w') as f:\n",
    "      for best_items_for_user in topk.eval():\n",
    "        f.write(','.join(str(x) for x in best_items_for_user) + '\\n')\n",
    "\n",
    "# online prediction returns row and column factors as needed\n",
    "def create_serving_input_fn(args):\n",
    "  def for_user_embeddings(userId):\n",
    "      # all items for this user (for user_embeddings)\n",
    "      items = tf.range(args['nitems'], dtype=tf.int64)\n",
    "      users = userId * tf.ones([args['nitems']], dtype=tf.int64)\n",
    "      ratings = 0.1 * tf.ones_like(users, dtype=tf.float32)\n",
    "      return items, users, ratings, tf.constant(True)\n",
    "    \n",
    "  def for_item_embeddings(itemId):\n",
    "      # all users for this item (for item_embeddings)\n",
    "      users = tf.range(args['nusers'], dtype=tf.int64)\n",
    "      items = itemId * tf.ones([args['nusers']], dtype=tf.int64)\n",
    "      ratings = 0.1 * tf.ones_like(users, dtype=tf.float32)\n",
    "      return items, users, ratings, tf.constant(False)\n",
    "    \n",
    "  def serving_input_fn():\n",
    "    feature_ph = {\n",
    "        'userId': tf.placeholder(tf.int64, 1),\n",
    "        'itemId': tf.placeholder(tf.int64, 1)\n",
    "    }\n",
    "\n",
    "    (items, users, ratings, project_row) = \\\n",
    "                  tf.cond(feature_ph['userId'][0] < tf.constant(0, dtype=tf.int64),\n",
    "                          lambda: for_item_embeddings(feature_ph['itemId']),\n",
    "                          lambda: for_user_embeddings(feature_ph['userId']))\n",
    "    rows = tf.stack( [users, items], axis=1 )\n",
    "    cols = tf.stack( [items, users], axis=1 )\n",
    "    input_rows = tf.SparseTensor(rows, ratings, (args['nusers'], args['nitems']))\n",
    "    input_cols = tf.SparseTensor(cols, ratings, (args['nusers'], args['nitems']))\n",
    "    \n",
    "    features = {\n",
    "      WALSMatrixFactorization.INPUT_ROWS: input_rows,\n",
    "      WALSMatrixFactorization.INPUT_COLS: input_cols,\n",
    "      WALSMatrixFactorization.PROJECT_ROW: project_row\n",
    "    }\n",
    "    return tf.contrib.learn.InputFnOps(features, None, feature_ph)\n",
    "  return serving_input_fn\n",
    "        \n",
    "def train_and_evaluate(args):\n",
    "    train_steps = int(0.5 + (1.0 * args['num_epochs'] * args['nusers']) / args['batch_size'])\n",
    "    steps_in_epoch = int(0.5 + args['nusers'] / args['batch_size'])\n",
    "    print('Will train for {} steps, evaluating once every {} steps'.format(train_steps, steps_in_epoch))\n",
    "    def experiment_fn(output_dir):\n",
    "        return tf.contrib.learn.Experiment(\n",
    "            tf.contrib.factorization.WALSMatrixFactorization(\n",
    "                         num_rows=args['nusers'], num_cols=args['nitems'],\n",
    "                         embedding_dimension=args['n_embeds'],\n",
    "                         model_dir=args['output_dir']),\n",
    "            train_input_fn=read_dataset(tf.estimator.ModeKeys.TRAIN, args),\n",
    "            eval_input_fn=read_dataset(tf.estimator.ModeKeys.EVAL, args),\n",
    "            train_steps=train_steps,\n",
    "            eval_steps=1,\n",
    "            min_eval_frequency=steps_in_epoch,\n",
    "            export_strategies=tf.contrib.learn.utils.saved_model_export_utils.make_export_strategy(serving_input_fn=create_serving_input_fn(args))\n",
    "        )\n",
    "\n",
    "    from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "    learn_runner.run(experiment_fn, args['output_dir'])\n",
    "    \n",
    "    batch_predict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train for 8 steps, evaluating once every 161 steps\n",
      "WARNING:tensorflow:From <ipython-input-23-bff0d06fd975>:86: run (from tensorflow.contrib.learn.python.learn.learn_runner) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.estimator.train_and_evaluate.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1179: __init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:427: __init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0aa0153b90>, '_model_dir': 'wals_trained', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "WARNING:tensorflow:From <ipython-input-23-bff0d06fd975>:82: make_export_strategy (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:483: __new__ (from tensorflow.contrib.learn.python.learn.export_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.estimator.train_and_evaluate, and use tf.estimator.Exporter.\n",
      "WARNING:tensorflow:From <ipython-input-23-bff0d06fd975>:82: __init__ (from tensorflow.contrib.learn.python.learn.experiment) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.estimator.train_and_evaluate. You will also have to convert to a tf.estimator.Estimator.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:279: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/factorization/python/ops/wals.py:315: __new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:SweepHook running init op.\n",
      "INFO:tensorflow:SweepHook running prep ops for the row sweep.\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into wals_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 97924.55, step = 1\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "INFO:tensorflow:Saving checkpoints for 8 into wals_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 103432.055.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-12-00:14:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from wals_trained/model.ckpt-8\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-12-00:14:29\n",
      "INFO:tensorflow:Saving dict for global step 8: global_step = 8, loss = 97924.55\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1373: get_timestamped_export_dir (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1378: get_temp_export_dir (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1388: get_input_alternatives (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1402: get_output_alternatives (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1412: build_all_signature_defs (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:267: build_standardized_signature_def (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "INFO:tensorflow:Restoring parameters from wals_trained/model.ckpt-8\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: wals_trained/export/Servo/temp-1555028070/saved_model.pb\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:480: garbage_collect_exports (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:393: largest_export_versions (from tensorflow.contrib.learn.python.learn.utils.gc) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please implement your own file management or use Saver.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:394: negation (from tensorflow.contrib.learn.python.learn.utils.gc) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please implement your own file management or use Saver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:396: get_paths (from tensorflow.contrib.learn.python.learn.utils.gc) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please implement your own file name management.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0a9b03fb90>, '_model_dir': 'wals_trained', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.rmtree('wals_trained', ignore_errors=True)\n",
    "train_and_evaluate({\n",
    "    'output_dir': 'wals_trained',\n",
    "    'input_path': 'data/',\n",
    "    'num_epochs': 0.05,\n",
    "    'nitems': 5668,\n",
    "    'nusers': 82802,\n",
    "\n",
    "    'batch_size': 512,\n",
    "    'n_embeds': 10,\n",
    "    'topk': 3\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_pred.txt\t\t\t\t     model.ckpt-1.data-00000-of-00001\r\n",
      "checkpoint\t\t\t\t     model.ckpt-1.index\r\n",
      "eval\t\t\t\t\t     model.ckpt-1.meta\r\n",
      "events.out.tfevents.1555028063.596f9bcfc81e  model.ckpt-8.data-00000-of-00001\r\n",
      "export\t\t\t\t\t     model.ckpt-8.index\r\n",
      "graph.pbtxt\t\t\t\t     model.ckpt-8.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls wals_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42,4913,5319\r\n",
      "2568,2904,2111\r\n",
      "3582,4930,5064\r\n",
      "4930,3582,3587\r\n",
      "3062,1309,2264\r\n",
      "4226,3515,1998\r\n",
      "5486,3305,2751\r\n",
      "4531,2730,2755\r\n",
      "4531,2730,2755\r\n",
      "2886,5406,2702\r\n"
     ]
    }
   ],
   "source": [
    "!head wals_trained/batch_pred.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as a Python module\n",
    "\n",
    "Let's run it as Python module for just a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train for 2 steps, evaluating once every 162 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING:tensorflow:From walsmodel/model.py:230: run (from tensorflow.contrib.learn.python.learn.learn_runner) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.estimator.train_and_evaluate.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1179: __init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:427: __init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f161334a290>, '_model_dir': '/content/datalab/notebooks/training-data-analyst/courses/machine_learning/deepdive/10_recommend/labs/wals_trained', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': u'cloud', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "WARNING:tensorflow:From walsmodel/model.py:226: make_export_strategy (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:483: __new__ (from tensorflow.contrib.learn.python.learn.export_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.estimator.train_and_evaluate, and use tf.estimator.Exporter.\n",
      "WARNING:tensorflow:From walsmodel/model.py:226: __init__ (from tensorflow.contrib.learn.python.learn.experiment) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.estimator.train_and_evaluate. You will also have to convert to a tf.estimator.Estimator.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:279: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/factorization/python/ops/wals.py:315: __new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:SweepHook running init op.\n",
      "INFO:tensorflow:SweepHook running prep ops for the row sweep.\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /content/datalab/notebooks/training-data-analyst/courses/machine_learning/deepdive/10_recommend/labs/wals_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 98078.0, step = 1\n",
      "INFO:tensorflow:Next fit step starting.\n",
      "2019-04-12 00:15:21.214121: W tensorflow/core/framework/allocator.cc:101] Allocation of 16916480 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Saving checkpoints for 2 into /content/datalab/notebooks/training-data-analyst/courses/machine_learning/deepdive/10_recommend/labs/wals_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 170587.38.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-12-00:15:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/notebooks/training-data-analyst/courses/machine_learning/deepdive/10_recommend/labs/wals_trained/model.ckpt-2\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-12-00:15:23\n",
      "INFO:tensorflow:Saving dict for global step 2: global_step = 2, loss = 98078.0\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1373: get_timestamped_export_dir (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1378: get_temp_export_dir (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1388: get_input_alternatives (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1402: get_output_alternatives (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1412: build_all_signature_defs (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:267: build_standardized_signature_def (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/notebooks/training-data-analyst/courses/machine_learning/deepdive/10_recommend/labs/wals_trained/model.ckpt-2\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /content/datalab/notebooks/training-data-analyst/courses/machine_learning/deepdive/10_recommend/labs/wals_trained/export/Servo/temp-1555028123/saved_model.pb\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:480: garbage_collect_exports (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:393: largest_export_versions (from tensorflow.contrib.learn.python.learn.utils.gc) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please implement your own file management or use Saver.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:394: negation (from tensorflow.contrib.learn.python.learn.utils.gc) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please implement your own file management or use Saver.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:396: get_paths (from tensorflow.contrib.learn.python.learn.utils.gc) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please implement your own file name management.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1612c69810>, '_model_dir': '/content/datalab/notebooks/training-data-analyst/courses/machine_learning/deepdive/10_recommend/labs/wals_trained', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': u'cloud', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf wals.tar.gz wals_trained\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=walsmodel.task \\\n",
    "   --package-path=${PWD}/walsmodel \\\n",
    "   -- \\\n",
    "   --output_dir=${PWD}/wals_trained \\\n",
    "   --input_path=${PWD}/data \\\n",
    "   --num_epochs=0.01 --nitems=5668 --nusers=82802 \\\n",
    "   --job-dir=./tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get row and column factors\n",
    "\n",
    "Once you have a trained WALS model, you can get row and column factors (user and item embeddings) using the serving input function that we exported.  We'll look at how to use these in the section on building a recommendation system using deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/input.json\n"
     ]
    }
   ],
   "source": [
    "%writefile data/input.json\n",
    "{\"userId\": 4, \"itemId\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECTION\n",
      "[-9.582241909811273e-05, 0.00026948447339236736, 0.0018090709345415235, -0.0001142259206972085, -0.000146866514114663, 0.001317993737757206, -0.000472685118438676, -0.00012022235750919208, 0.0007437687017954886, 0.0007974716136232018]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2019-04-12 00:15:45.133944: W tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc:1441] Failed to build SimpleGraphView.\n",
      "2019-04-12 00:15:45.136668: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:586] Iteration = 0, topological sort failed with message: Non-existent input ^ConstantFoldingCtrl/cond_1/cond_1/ones/Less/Switch_1 for node ConstantFolding/cond_1/cond_1/ones/packed_const_axis\n",
      "2019-04-12 00:15:45.137790: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:586] Iteration = 1, topological sort failed with message: Non-existent input ^ConstantFoldingCtrl/cond_1/cond_1/ones/Less/Switch_1 for node ConstantFolding/cond_1/cond_1/ones/packed_const_axis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_DIR=$(ls wals_trained/export/Servo | tail -1)\n",
    "gcloud ml-engine local predict --model-dir=wals_trained/export/Servo/$MODEL_DIR --json-instances=data/input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/input.json\n"
     ]
    }
   ],
   "source": [
    "%writefile data/input.json\n",
    "{\"userId\": -1, \"itemId\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECTION\n",
      "[-0.0006948178051970899, -8.258508023573086e-05, 5.338675691746175e-05, 0.00047609733883291483, 0.00045299611520022154, -4.492652806220576e-06, -0.00025688076857477427, -0.000267045630607754, -0.00010975753684761003, 0.0007525525288656354]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2019-04-12 00:15:59.661649: W tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc:1441] Failed to build SimpleGraphView.\n",
      "2019-04-12 00:15:59.664291: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:586] Iteration = 0, topological sort failed with message: Non-existent input ^ConstantFoldingCtrl/cond_1/cond_1/ones/Less/Switch_1 for node ConstantFolding/cond_1/cond_1/ones/packed_const_axis\n",
      "2019-04-12 00:15:59.665291: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:586] Iteration = 1, topological sort failed with message: Non-existent input ^ConstantFoldingCtrl/cond_1/cond_1/ones/Less/Switch_1 for node ConstantFolding/cond_1/cond_1/ones/packed_const_axis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_DIR=$(ls wals_trained/export/Servo | tail -1)\n",
    "gcloud ml-engine local predict --model-dir=wals_trained/export/Servo/$MODEL_DIR --json-instances=data/input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://data/collab_mapped.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/  7.4 MiB]                                                \r",
      "Copying file://data/collab_raw.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/ 19.9 MiB]                                                \r",
      "Copying file://data/input.json [Content-Type=application/json]...\n",
      "/ [0 files][    0.0 B/ 19.9 MiB]                                                \r",
      "Copying file://data/items.csv [Content-Type=text/csv]...\n",
      "/ [0/8 files][    0.0 B/ 31.1 MiB]   0% Done                                    \r",
      "Copying file://data/items_for_user [Content-Type=application/octet-stream]...\n",
      "/ [0/8 files][    0.0 B/ 31.1 MiB]   0% Done                                    \r",
      "-\r",
      "- [1/8 files][ 80.3 KiB/ 31.1 MiB]   0% Done                                    \r",
      "Copying file://data/items_for_user_subset [Content-Type=application/octet-stream]...\n",
      "- [1/8 files][ 80.3 KiB/ 31.1 MiB]   0% Done                                    \r",
      "- [2/8 files][ 80.3 KiB/ 31.1 MiB]   0% Done                                    \r",
      "Copying file://data/users.csv [Content-Type=text/csv]...\n",
      "- [2/8 files][608.3 KiB/ 31.1 MiB]   1% Done                                    \r",
      "\\\r",
      "\\ [3/8 files][  5.5 MiB/ 31.1 MiB]  17% Done                                    \r",
      "Copying file://data/users_for_item [Content-Type=application/octet-stream]...\n",
      "\\ [3/8 files][  6.3 MiB/ 31.1 MiB]  20% Done                                    \r",
      "\\ [4/8 files][ 22.7 MiB/ 31.1 MiB]  73% Done                                    \r",
      "|\r",
      "| [5/8 files][ 26.0 MiB/ 31.1 MiB]  83% Done                                    \r",
      "| [6/8 files][ 28.1 MiB/ 31.1 MiB]  90% Done                                    \r",
      "| [7/8 files][ 30.9 MiB/ 31.1 MiB]  99% Done                                    \r",
      "| [8/8 files][ 31.1 MiB/ 31.1 MiB] 100% Done                                    \r\n",
      "Operation completed over 8 objects/31.1 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil -m cp data/* gs://${BUCKET}/wals/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-ef189eb62756ef5e/wals/model_trained us-central1 wals_190412_001615\n",
      "jobId: wals_190412_001615\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [wals_190412_001615] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe wals_190412_001615\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs wals_190412_001615\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/wals/model_trained\n",
    "JOBNAME=wals_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=walsmodel.task \\\n",
    "   --package-path=${PWD}/walsmodel \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC_GPU \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   -- \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --input_path=gs://${BUCKET}/wals/data \\\n",
    "   --num_epochs=10 --nitems=5668 --nusers=82802 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2019-04-12T00:16:32Z'\r\n",
      "endTime: '2019-04-12T00:30:49Z'\r\n",
      "etag: NgmqG_3mI4I=\r\n",
      "jobId: wals_190412_001615\r\n",
      "startTime: '2019-04-12T00:18:02Z'\r\n",
      "state: SUCCEEDED\r\n",
      "trainingInput:\r\n",
      "  args:\r\n",
      "  - --output_dir=gs://qwiklabs-gcp-ef189eb62756ef5e/wals/model_trained\r\n",
      "  - --input_path=gs://qwiklabs-gcp-ef189eb62756ef5e/wals/data\r\n",
      "  - --num_epochs=10\r\n",
      "  - --nitems=5668\r\n",
      "  - --nusers=82802\r\n",
      "  jobDir: gs://qwiklabs-gcp-ef189eb62756ef5e/wals/model_trained\r\n",
      "  packageUris:\r\n",
      "  - gs://qwiklabs-gcp-ef189eb62756ef5e/wals_190412_001615/dd7b79e57a9592dca68e3ea2c1944b043cc4e244778965876a76ea485f0c86f4/walsmodel-0.0.0.tar.gz\r\n",
      "  pythonModule: walsmodel.task\r\n",
      "  region: us-central1\r\n",
      "  runtimeVersion: '1.8'\r\n",
      "  scaleTier: BASIC_GPU\r\n",
      "trainingOutput:\r\n",
      "  consumedMLUnits: 0.28\r\n",
      "\r\n",
      "View job in the Cloud Console at:\r\n",
      "https://console.cloud.google.com/ml/jobs/wals_190412_001615?project=qwiklabs-gcp-ef189eb62756ef5e\r\n",
      "\r\n",
      "View logs at:\r\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2Fwals_190412_001615&project=qwiklabs-gcp-ef189eb62756ef5e\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine jobs describe wals_190412_001615"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took <b>10 minutes</b> for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "# Copyright 2018 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
